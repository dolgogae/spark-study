{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8bd92-63e3-419f-8a9a-ff3204c63779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run -it --rm -p 8888:8888 \\\n",
    "#     -v /Users/seungjoonlee/git/pyspark:/home/jovyan/work \\\n",
    "#     --user root \\\n",
    "#     -e NB_GID=100 \\\n",
    "#     -e GRANT_SUDO=yes \\\n",
    "#     -e GRANT_SUDO=yes jupyter/pyspark-notebook\n",
    "\n",
    "# # # apt update\n",
    "# # apt update\n",
    "\n",
    "# # # netcat install\n",
    "# # apt-get install netcat\n",
    "\n",
    "# # # start netcat server\n",
    "# # nc -lk 9999\n",
    "\n",
    "# # # # run the sparkstreaming in terminal\n",
    "# # # /usr/local/spark/bin/spark-submit /home/jovyan/work/spark_streaming.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a65d3c-ff6f-4acb-9e01-0b5752ce3112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-07-23 08:11:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-07-23 08:11:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-07-23 08:11:20\n",
      "-------------------------------------------\n",
      "('ㅗ�Hello', 1)\n",
      "('kafka', 1)\n",
      "('Hello', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-07-23 08:11:30\n",
      "-------------------------------------------\n",
      "('Hello', 1)\n",
      "('spark', 1)\n",
      "('Cat', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-07-23 08:11:40\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58960)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 5 seconds\n",
    "# local[n]은 코어를 n개 사용한다는 뜻.\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "# 10 sec interval\n",
    "# 10초안에 소켓으로 들어온 데이터를 읽는다.\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"127.0.0.1\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "# ssc.awaitTermination()  # Wait for the computation to terminate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c5cf3-7e59-4e37-bde3-be8f205ea23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
